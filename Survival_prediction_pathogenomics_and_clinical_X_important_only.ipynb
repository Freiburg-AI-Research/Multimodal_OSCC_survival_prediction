{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcpZlpaIin55"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "# Load the file as a Pandas DataFrame\n",
        "data = pd.read_excel(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\R\\Pathogenomics_dataset_1_Copy.xlsx\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrln2yP_in56"
      },
      "source": [
        "|**Descriptive Statistics of the dataset**|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAb8FMy_in57"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_vars = data.select_dtypes(include=[np.number]).columns\n",
        "cat_vars = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "\n",
        "# Create an empty table\n",
        "table = pd.DataFrame(columns=['Variable', 'Mean±SD', 'Count (%)'])\n",
        "\n",
        "# Iterate over each numerical variable and get the mean and standard deviation\n",
        "num_cols = data.select_dtypes(include=['float', 'int']).columns\n",
        "for col in num_cols:\n",
        "    mean = data[col].mean()\n",
        "    std = data[col].std()\n",
        "    table = table.append({'Variable': col, 'Mean ± SD': f\"{mean:.2f} ± {std:.2f}\", 'Count (%)': ''}, ignore_index=True)\n",
        "\n",
        "# Iterate over each categorical variable and get the count and percentage of each category\n",
        "cat_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
        "for col in cat_cols:\n",
        "    categories = data[col].unique().tolist()\n",
        "    if len(categories) == 1:\n",
        "        count = data[col].count()\n",
        "        percent = count / len(data) * 100\n",
        "        table = table.append({'Variable': col, 'Mean ± SD': '', 'Count (%)': f\"{count} ({percent:.2f}%)\"},\n",
        "                             ignore_index=True)\n",
        "    else:\n",
        "        for category in categories:\n",
        "            count = data[data[col] == category][col].count()\n",
        "            percent = count / len(data) * 100\n",
        "            table = table.append({'Variable': f\"{col} - {category}\", 'Mean ± SD': '',\n",
        "                                  'Count (%)': f\"{count} ({percent:.2f}%)\"},\n",
        "                                 ignore_index=True)\n",
        "\n",
        "# Save the table to an Excel file\n",
        "table.to_excel('table.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSHCiwc4in58"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from lifelines import KaplanMeierFitter\n",
        "from lifelines.plotting import add_at_risk_counts\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "kmfh = KaplanMeierFitter()\n",
        "kmfh.fit(data['overall_survival'], event_observed=data['vital_status'])\n",
        "kmfh.plot(ax=ax, show_censors =True)\n",
        "plt.ylim(0, 1.01)\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Probalities\")\n",
        "plt.legend(loc=\"best\")\n",
        "add_at_risk_counts(kmfh, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1L24Dpwin58"
      },
      "source": [
        "|**Main analyses**|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgfhGwZqin59"
      },
      "outputs": [],
      "source": [
        "data_processed = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC-OSuF2in59"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "# separate the numerical and categorical variables\n",
        "num_vars = data_processed.select_dtypes(include=[np.number]).columns\n",
        "cat_vars = data_processed.select_dtypes(include=['object']).columns\n",
        "\n",
        "# impute missing values for numerical variables with median\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "data_processed[num_vars] = num_imputer.fit_transform(data_processed[num_vars])\n",
        "\n",
        "# impute missing values for categorical variables with most frequent\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "data_processed[cat_vars] = cat_imputer.fit_transform(data_processed[cat_vars])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oagYzctnin59"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Convert string categorical variables to numeric labels\n",
        "categorical_cols = [\"ajcc_pathologic_stage\", \"ajcc_clinical_stage\", \"prior_malignancy\", \"prior_treatment\", \"ajcc_pathologic_t\", \"morphology\", \"ajcc_clinical_m\", \"ajcc_pathologic_n\", \"ajcc_clinical_n\", \"ajcc_clinical_t\", \"ajcc_pathologic_m\", \"alcohol_history\", \"race\", \"gender\", \"ethnicity\", \"paper_Methylation\"]\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    data_processed[col] = label_encoders[col].fit_transform(data_processed[col])\n",
        "\n",
        "# Convert integer columns to float64\n",
        "data_processed = data_processed.astype({col: 'float64' for col in data_processed.select_dtypes(include='int32').columns})\n",
        "\n",
        "# Standardize the continuous variables\n",
        "continuous_cols = data_processed.columns.difference(categorical_cols + ['vital_status'] + ['overall_survival'])\n",
        "scaler = StandardScaler()\n",
        "data_processed[continuous_cols] = scaler.fit_transform(data_processed[continuous_cols])\n",
        "\n",
        "\n",
        "\n",
        "# Verify that the data has been preprocessed correctly\n",
        "print(data_processed.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXAjgtJ8in59"
      },
      "outputs": [],
      "source": [
        "data_processed.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpfZIuEFin59"
      },
      "outputs": [],
      "source": [
        "non_numeric_cols = data_processed.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
        "print(non_numeric_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNmhFry5in59"
      },
      "outputs": [],
      "source": [
        "data_processed.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbMjEiiWin59"
      },
      "outputs": [],
      "source": [
        "X = data_processed.drop([\"overall_survival\", \"vital_status\"], axis=1)\n",
        "y = data_processed[['vital_status','overall_survival']]\n",
        "y['vital_status'] = y['vital_status'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txo5n0mBin5-"
      },
      "outputs": [],
      "source": [
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpZedv9jin5-"
      },
      "outputs": [],
      "source": [
        "print(X.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7age1LnQin5-"
      },
      "outputs": [],
      "source": [
        "non_numeric_columns = []\n",
        "for column in X.columns:\n",
        "    if X[column].dtype != 'float64' and X[column].dtype != 'int64':\n",
        "        non_numeric_columns.append(column)\n",
        "        \n",
        "if len(non_numeric_columns) > 0:\n",
        "    print(\"Non-numeric columns found: \", non_numeric_columns)\n",
        "else:\n",
        "    print(\"All columns are numeric\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVEBxKPlin5-"
      },
      "outputs": [],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXm4lBxTin5-"
      },
      "outputs": [],
      "source": [
        "print(y['vital_status'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQAGhYGkin5-"
      },
      "outputs": [],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xa16XLxgin5-"
      },
      "outputs": [],
      "source": [
        "for col in data_processed.columns:\n",
        "    if data_processed[col].isnull().any():\n",
        "        print(f\"{col} has missing values\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlgkEJqDin5-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sksurv.ensemble import RandomSurvivalForest\n",
        "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
        "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sksurv.util import Surv\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sksurv.ensemble import RandomSurvivalForest\n",
        "from lifelines.utils import survival_events_from_table\n",
        "from sksurv.svm import FastSurvivalSVM\n",
        "from sklearn.metrics import make_scorer\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "from functools import partial, wraps\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from sksurv.metrics import (concordance_index_censored, concordance_index_ipcw,\n",
        "                            brier_score, cumulative_dynamic_auc)\n",
        "from sksurv.metrics import brier_score, integrated_brier_score\n",
        "from sklearn.metrics import brier_score_loss\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.seterr(over='ignore')\n",
        "\n",
        "# Define loss function for deep learning model\n",
        "def coxph_loss(y_true, y_pred):\n",
        "    # extract the risk scores and event indicators\n",
        "    rs = y_pred[:, 0]\n",
        "    ei = y_true[:, 1]\n",
        "    # calculate the log partial likelihood\n",
        "    hazard_ratio = K.exp(rs)\n",
        "    log_risk = K.log(K.cumsum(hazard_ratio))\n",
        "    uncensored_likelihood = rs - log_risk\n",
        "    censored_likelihood = uncensored_likelihood * ei\n",
        "    neg_likelihood = -K.sum(censored_likelihood, axis=-1)\n",
        "    return neg_likelihood\n",
        "\n",
        "# Define function to create deep survival model\n",
        "def create_deep_survival_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, input_shape=(input_dim,), activation=\"relu\"))\n",
        "    model.add(Dense(2, activation=\"softmax\"))\n",
        "    model.compile(optimizer=\"adam\", loss=coxph_loss)\n",
        "    return model\n",
        "\n",
        "# Define models\n",
        "models = [\n",
        "    (\"RSF\", RandomSurvivalForest(n_estimators=100, max_depth=5, random_state=42)),\n",
        "    (\"GBSA\", GradientBoostingSurvivalAnalysis(n_estimators=100, max_depth=5, random_state=42)),\n",
        "    (\"FastSVM\", FastSurvivalSVM(alpha=1.0, max_iter=1000, random_state=42)),\n",
        "    (\"CoxPH\", CoxPHSurvivalAnalysis(alpha=0.5, verbose=False)),\n",
        "    (\"DeepSurv\", create_deep_survival_model)\n",
        "]\n",
        "\n",
        "# Define k-fold cross-validation\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "def survival_events_from_table(df, event_col, time_col):\n",
        "    \"\"\"\n",
        "    Convert a DataFrame to the event and time format expected by DeepSurv.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame with columns for event indicator and time.\n",
        "    event_col : str\n",
        "        Name of the event column.\n",
        "    time_col : str\n",
        "        Name of the time column.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        Tuple of (event_indicator, observed_time) arrays.\n",
        "\n",
        "    \"\"\"\n",
        "    df = df.set_index(time_col)\n",
        "    event_indicator = df[event_col].values.astype(bool)\n",
        "    observed_time = df.index.values.astype(float)\n",
        "    return event_indicator, observed_time\n",
        "\n",
        "\n",
        "# Run k-fold cross-validation\n",
        "for model_name, model in models:\n",
        "    print(model_name)\n",
        "    c_indexes = []\n",
        "    #X_important was derived from the feature importance analyses i.e. top 10 predictors see code below. run code below before this otherwise this does not work and should be X.iloc and kf.split(X).so X_important should be replaced with X if not using the top 10 predictors but the whole feature set\n",
        "    for train_index, test_index in kf.split(X_important):\n",
        "        X_train, y_train = X_important.iloc[train_index], y.iloc[train_index]\n",
        "        X_test, y_test = X_important.iloc[test_index], y.iloc[test_index]\n",
        "\n",
        "        if model_name == \"DeepSurv\":\n",
        "            # transform y_train using StandardScaler\n",
        "            scaler = StandardScaler()\n",
        "            y_train_censored = survival_events_from_table(y_train, 'vital_status', 'overall_survival')\n",
        "            y_train_transformed_1 = scaler.fit_transform(y_train_censored[0].reshape(-1, 1)).flatten()\n",
        "            y_train_transformed_2 = y_train_censored[1].reshape(-1, 1)\n",
        "            y_train_transformed = np.concatenate([y_train_transformed_1.reshape(-1, 1), y_train_transformed_2], axis=1)\n",
        "            y_train_transformed = pd.DataFrame(y_train_transformed, columns=['T', 'E'])\n",
        "            # transform y_test using the same scaler\n",
        "            y_test_censored = survival_events_from_table(y_test, 'vital_status', 'overall_survival')\n",
        "            y_test_transformed_1 = scaler.transform(y_test_censored[0].reshape(-1, 1)).flatten()\n",
        "            y_test_transformed_2 = y_test_censored[1].reshape(-1, 1)\n",
        "            y_test_transformed = np.concatenate([y_test_transformed_1.reshape(-1, 1), y_test_transformed_2], axis=1)\n",
        "            y_test_transformed = pd.DataFrame(y_test_transformed, columns=['T', 'E'])\n",
        "            model_instance = KerasRegressor(build_fn=create_deep_survival_model, input_dim=X_train.shape[1], epochs=50, verbose=0) # instantiate the model\n",
        "            model_instance.fit(X_train, y_train_transformed)  # fit the model\n",
        "            y_pred = model_instance.predict(X_test)\n",
        "            y_pred_risk = np.exp(-np.dot(y_pred, model_instance.model.layers[-1].get_weights()[0].T) - model_instance.model.layers[-1].get_weights()[1][0])\n",
        "            # Compute predicted risk\n",
        "            y_pred_log_odds = np.dot(y_pred, model_instance.model.layers[-1].get_weights()[0].T) + model_instance.model.layers[-1].get_weights()[1][0]\n",
        "            y_pred_risk = 1 / (1 + np.exp(-y_pred_log_odds))\n",
        "            y_pred_risk = np.mean(y_pred_risk, axis=1)\n",
        "            c_index = concordance_index_censored(y_test['vital_status'].astype(bool), y_test['overall_survival'], np.ravel(y_pred[:, 0]))[0]\n",
        "            c_indexes.append(c_index)\n",
        "            print(f\"Fold c-index: {c_index:.3f}\")\n",
        "            print(f\"Mean c-index: {np.mean(c_indexes):.3f}\\n\")\n",
        "            # Set threshold on predicted risk score\n",
        "            threshold = 0.5\n",
        "            y_pred_binary = np.zeros_like(y_pred_risk)\n",
        "            y_pred_binary[y_pred_risk > threshold] = 1\n",
        "\n",
        "            # Calculate ROC curve and AUC\n",
        "            fpr, tpr, thresholds = roc_curve(y_test['vital_status'], y_pred_risk)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            # Plot ROC curve\n",
        "            plt.figure(figsize=(8, 8))\n",
        "            plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "            plt.xlim([0.0, 1.0])\n",
        "            plt.ylim([0.0, 1.05])\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title('Receiver operating characteristic')\n",
        "            plt.legend(loc=\"lower right\")\n",
        "            plt.show()\n",
        "\n",
        "            print('\\n')\n",
        "\n",
        "            # Calculate AUC at specific time points\n",
        "            y_pred_df = pd.DataFrame(y_pred_risk, index=y_test.index, columns=['survival_prob'])\n",
        "            pool1year = y_test['overall_survival'] <= 365\n",
        "            pool2year = y_test['overall_survival'] <= 730\n",
        "            y_true_1year = y_test.loc[pool1year, 'vital_status']\n",
        "            y_pred_1year = y_pred_df.loc[pool1year, 'survival_prob']\n",
        "            fpr_1year, tpr_1year, thresholds_1year = roc_curve(y_true_1year, y_pred_1year)\n",
        "            roc_auc_1year = auc(fpr_1year, tpr_1year)\n",
        "            auc_list_1year_deepsurv = []\n",
        "            auc_list_1year_deepsurv.append(roc_auc_1year)\n",
        "            print(f\"AUC after 1 year: {roc_auc_1year:.3f}\")\n",
        "            plt.plot(fpr_1year, tpr_1year, label=f\"ROC Curve after 1 year (AUC = {roc_auc_1year:.3f})\")\n",
        "\n",
        "            y_true_2year = y_test.loc[pool2year, 'vital_status']\n",
        "            y_pred_2year = y_pred_df.loc[pool2year, 'survival_prob']\n",
        "            fpr_2year, tpr_2year, thresholds_2year = roc_curve(y_true_2year, y_pred_2year)\n",
        "            roc_auc_2year = auc(fpr_2year, tpr_2year)\n",
        "            auc_list_2year_deepsurv = []\n",
        "            auc_list_2year_deepsurv.append(roc_auc_2year)\n",
        "\n",
        "            print(f\"AUC after 2 years: {roc_auc_2year:.3f}\")\n",
        "            plt.plot(fpr_2year, tpr_2year, label=f\"ROC Curve after 2 years (AUC = {roc_auc_2year:.3f})\")\n",
        "\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title(f'ROC Curves - {model_name}')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "            # calculate the average AUC and the standard deviation over all folds\n",
        "            avg_auc1deepsurv = np.mean(auc_list_1year_deepsurv)\n",
        "            std_auc1deepsurv = np.std(auc_list_1year_deepsurv)\n",
        "            avg_auc2deepsurv = np.mean(auc_list_2year_deepsurv)\n",
        "            std_auc2deepsurv = np.std(auc_list_2year_deepsurv)\n",
        "            # print the results\n",
        "            print(f\"Average AUC 1 year over {n_splits} folds: {avg_auc1deepsurv:.3f} +/- {std_auc1deepsurv:.3f}\")\n",
        "            print(f\"Average AUC 2 year over {n_splits} folds: {avg_auc2deepsurv:.3f} +/- {std_auc2deepsurv:.3f}\")\n",
        "\n",
        "\n",
        "        else:\n",
        "            y_train_struct = np.zeros(y_train.shape[0], dtype={'names':['vital_status', 'overall_survival'], 'formats':['?','f8']})\n",
        "            y_train_struct['overall_survival'] = y_train['overall_survival']\n",
        "            y_train_struct['vital_status'] = y_train['vital_status']\n",
        "            model_instance = model\n",
        "            model_instance.fit(X_train, y_train_struct)\n",
        "            y_pred = model_instance.predict(X_test)\n",
        "            y_test = y_test.copy()\n",
        "            y_test.loc[:, 'vital_status'] = y_test['vital_status'].astype(bool)\n",
        "            c_index = concordance_index_censored(y_test['vital_status'], y_test['overall_survival'], y_pred)[0]\n",
        "            c_indexes.append(c_index)\n",
        "            print(f\"Fold c-index: {c_index:.3f}\")\n",
        "            print(f\"Mean c-index: {np.mean(c_indexes):.3f}\\n\")\n",
        "            threshold = np.median(y_test['overall_survival'])\n",
        "            y_true = (y_test['overall_survival'] > threshold).astype(int)\n",
        "            fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            print(roc_auc)\n",
        "            plt.plot(fpr, tpr)\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title(f'ROC Curve - {model_name}')\n",
        "            plt.show()\n",
        "            \n",
        "            print('\\n')\n",
        "            # Calculate AUC at specific time points\n",
        "            y_pred_df = pd.DataFrame(y_pred, index=y_test.index, columns=['survival_prob'])\n",
        "            pool1year = y_test['overall_survival'] <= 365\n",
        "            pool2year = y_test['overall_survival'] <= 730\n",
        "            y_true_1year = y_test.loc[pool1year, 'vital_status']\n",
        "            y_pred_1year = y_pred_df.loc[pool1year, 'survival_prob']\n",
        "            fpr_1year, tpr_1year, thresholds_1year = roc_curve(y_true_1year, y_pred_1year)\n",
        "            roc_auc_1year = auc(fpr_1year, tpr_1year)\n",
        "            auc_list_1year = []\n",
        "            auc_list_1year.append(roc_auc_1year)\n",
        "            print(f\"AUC after 1 year: {roc_auc_1year:.3f}\")\n",
        "            plt.plot(fpr_1year, tpr_1year, label=f\"ROC Curve after 1 year (AUC = {roc_auc_1year:.3f})\")\n",
        "\n",
        "            y_true_2year = y_test.loc[pool2year, 'vital_status']\n",
        "            y_pred_2year = y_pred_df.loc[pool2year, 'survival_prob']\n",
        "            fpr_2year, tpr_2year, thresholds_2year = roc_curve(y_true_2year, y_pred_2year)\n",
        "            roc_auc_2year = auc(fpr_2year, tpr_2year)\n",
        "            auc_list_2year = []\n",
        "            auc_list_2year.append(roc_auc_2year)\n",
        "\n",
        "            print(f\"AUC after 2 years: {roc_auc_2year:.3f}\")\n",
        "            plt.plot(fpr_2year, tpr_2year, label=f\"ROC Curve after 2 years (AUC = {roc_auc_2year:.3f})\")\n",
        "\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title(f'ROC Curves - {model_name}')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "            # calculate the average AUC and the standard deviation over all folds\n",
        "            avg_auc1 = np.mean(auc_list_1year)\n",
        "            std_auc1 = np.std(auc_list_1year)\n",
        "            avg_auc2 = np.mean(auc_list_2year)\n",
        "            std_auc2 = np.std(auc_list_2year)\n",
        "            # print the results\n",
        "            print(f\"Average 1 year AUC over {n_splits} folds: {avg_auc1:.3f} +/- {std_auc1:.3f}\")\n",
        "            print(f\"Average 2 year AUC over {n_splits} folds: {avg_auc2:.3f} +/- {std_auc2:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0em37g06in5_"
      },
      "source": [
        "|**Loss for deepcurve**|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dxv4eU9in5_"
      },
      "outputs": [],
      "source": [
        "history = model_instance.fit(X_train, y_train_transformed, validation_data=(X_test, y_test_transformed), epochs=50, verbose=0)\n",
        "\n",
        "# Get the train and validation loss values from DeepSurv\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Plot the train and validation loss curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_loss, label='train_loss')\n",
        "plt.plot(val_loss, label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0XLRkmein5_"
      },
      "source": [
        "|**ROC for deepcurve**|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBmkP9Anin5_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# Set threshold on predicted risk score\n",
        "threshold = 0.1\n",
        "y_pred_binary = np.zeros_like(y_pred_risk)\n",
        "y_pred_binary[y_pred_risk > threshold] = 1\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test['vital_status'], y_pred_risk)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Calculate calibration curve\n",
        "prob_true, prob_pred = calibration_curve(y_test['vital_status'], y_pred_risk, n_bins=10)\n",
        "\n",
        "# Plot calibration curve\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
        "plt.plot(prob_pred, prob_true, \"s-\", label=\"Model\")\n",
        "plt.xlabel(\"Predicted probability\")\n",
        "plt.ylabel(\"True probability\")\n",
        "plt.legend()\n",
        "plt.title('Calibration curve')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4oqsNt8in5_"
      },
      "source": [
        "|**Feature Importance**|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCXWw0GJin5_"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "for model_name, model in models:\n",
        "    print(model_name)\n",
        "    if model_name == \"DeepSurv\":\n",
        "        # Calculate feature importances\n",
        "        print(type(model))\n",
        "        result = permutation_importance(model_instance, X_train, y_train_transformed, n_repeats=10, random_state=0)\n",
        "\n",
        "        # Get feature importance scores and their indices\n",
        "        importance_scores = result.importances_mean\n",
        "        importance_indices = np.argsort(importance_scores)[::-1]\n",
        "\n",
        "        # Print the feature importances\n",
        "        for i in importance_indices:\n",
        "            print(f\"{X_train.columns[i]}: {importance_scores[i]}\")\n",
        "    else:\n",
        "        # Calculate feature importances\n",
        "        result = permutation_importance(model, X_train, y_train_struct, n_repeats=10, random_state=0)\n",
        "\n",
        "        # Get feature importance scores and their indices\n",
        "        importance_scores = result.importances_mean\n",
        "        importance_indices = np.argsort(importance_scores)[::-1]\n",
        "\n",
        "        # Print the feature importances\n",
        "        for i in importance_indices:\n",
        "            print(f\"{X_train.columns[i]}: {importance_scores[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj68E8bKin5_"
      },
      "source": [
        "|**Heatmap for feature importance seperately**|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYQMT7GIin5_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "for model_name, model in models:\n",
        "    print(model_name)\n",
        "    if model_name == \"DeepSurv\":\n",
        "        # Calculate feature importances\n",
        "        result = permutation_importance(model_instance, X_train, y_train_transformed, n_repeats=10, random_state=0)\n",
        "\n",
        "        # Get feature importance scores and their indices\n",
        "        importance_scores = result.importances_mean\n",
        "        importance_indices = np.argsort(importance_scores)[::-1]\n",
        "\n",
        "        # Calculate the baseline C-index\n",
        "        baseline = concordance_index_censored(y_test['vital_status'].astype(bool), y_pred_df['survival_prob'], y_test['overall_survival'])[0]\n",
        "\n",
        "\n",
        "        # Calculate the percentage reduction in C-index for each feature\n",
        "        importance_scores_pct = 100 * (baseline - result.importances_mean) / baseline\n",
        "\n",
        "        # Create a heatmap of feature importances\n",
        "        sns.set(rc={\"figure.figsize\":(15,10)})\n",
        "        sns.set(font_scale=1.2)\n",
        "        ax = sns.heatmap(importance_scores_pct.reshape(1, -1), annot=False, cmap=\"YlOrRd\", fmt=\".1f\", cbar=False, xticklabels=False, yticklabels=False)\n",
        "        #ax = sns.heatmap(importance_scores_pct.reshape(1, -1), annot=True, cmap=\"YlOrRd\", fmt=\".1f\", cbar=False, xticklabels=X_train.columns[importance_indices], yticklabels=False)\n",
        "        ax.set_title(f\"Feature importance for {model_name}\")\n",
        "        plt.show()\n",
        "        \n",
        "    else:\n",
        "        # Calculate feature importances\n",
        "        result = permutation_importance(model, X_train, y_train_struct, n_repeats=10, random_state=0)\n",
        "\n",
        "        # Get feature importance scores and their indices\n",
        "        importance_scores = result.importances_mean\n",
        "        importance_indices = np.argsort(importance_scores)[::-1]\n",
        "\n",
        "        # Calculate the baseline C-index\n",
        "        baseline = concordance_index_censored(y_test['vital_status'].astype(bool), y_pred_df['survival_prob'], y_test['overall_survival'])[0]\n",
        "\n",
        "        # Calculate the percentage reduction in C-index for each feature\n",
        "        importance_scores_pct = 100 * (baseline - result.importances_mean) / baseline\n",
        "\n",
        "        # Create a heatmap of feature importances\n",
        "        sns.set(rc={\"figure.figsize\":(15,10)})\n",
        "        sns.set(font_scale=1.2)\n",
        "        ax = sns.heatmap(importance_scores_pct.reshape(1, -1), annot=False, cmap=\"YlOrRd\", fmt=\".1f\", cbar=False, xticklabels=False, yticklabels=False)\n",
        "        #ax = sns.heatmap(importance_scores_pct.reshape(1, -1), annot=False, cmap=\"YlOrRd\", fmt=\".1f\", cbar=False, xticklabels=X_train.columns[importance_indices], yticklabels=False)\n",
        "        ax.set_title(f\"Feature importance for {model_name}\")\n",
        "        \n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPClOlYzin6A"
      },
      "source": [
        "|**Heatmap for feature importance pooled**|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz4NWRj1in6A"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Create a dictionary to store the importance scores for each model\n",
        "importance_scores_dict = {}\n",
        "\n",
        "for model_name, model in models:\n",
        "    \n",
        "    print(model_name)\n",
        "    if model_name == \"DeepSurv\":\n",
        "        # Calculate feature importances\n",
        "        result = permutation_importance(model_instance, X_train, y_train_transformed, n_repeats=10, random_state=0)\n",
        "\n",
        "        # Get feature importance scores and their indices\n",
        "        importance_scores = result.importances_mean\n",
        "        importance_indices = np.argsort(importance_scores)[::-1]\n",
        "\n",
        "        # Calculate the baseline C-index\n",
        "        baseline = concordance_index_censored(y_test['vital_status'].astype(bool), y_pred_df['survival_prob'], y_test['overall_survival'])[0]\n",
        "\n",
        "        # Calculate the percentage reduction in C-index for each feature\n",
        "        importance_scores_pct = 100 * (baseline - result.importances_mean) / baseline\n",
        "\n",
        "        # Min-max scaling of importance scores between 0 and 1\n",
        "        importance_scores_norm = (importance_scores_pct - np.min(importance_scores_pct)) / (np.max(importance_scores_pct) - np.min(importance_scores_pct))\n",
        "        \n",
        "        # Add importance scores to the importance matrices dictionary\n",
        "        importance_scores_dict[model_name] = importance_scores_norm\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Calculate feature importances\n",
        "        result = permutation_importance(model, X_train, y_train_struct, n_repeats=10, random_state=0)\n",
        "\n",
        "        # Get feature importance scores and their indices\n",
        "        importance_scores = result.importances_mean\n",
        "        importance_indices = np.argsort(importance_scores)[::-1]\n",
        "\n",
        "        # Calculate the baseline C-index\n",
        "        baseline = concordance_index_censored(y_test['vital_status'].astype(bool), y_pred_df['survival_prob'], y_test['overall_survival'])[0]\n",
        "\n",
        "        # Calculate the percentage reduction in C-index for each feature\n",
        "        importance_scores_pct = 100 * (baseline - result.importances_mean) / baseline\n",
        "\n",
        "        # Min-max scaling of importance scores between 0 and 1\n",
        "        importance_scores_norm = (importance_scores_pct - np.min(importance_scores_pct)) / (np.max(importance_scores_pct) - np.min(importance_scores_pct))\n",
        "\n",
        "        # Add the importance scores to the dictionary\n",
        "        importance_scores_dict[model_name] = importance_scores_norm\n",
        "\n",
        "# Concatenate the importance scores for all models\n",
        "all_importance_scores = np.concatenate(list(importance_scores_dict.values()))\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = X_train.columns.values\n",
        "\n",
        "# Create a heatmap of feature importances for all models\n",
        "sns.set(rc={\"figure.figsize\":(15,10)})\n",
        "sns.set(font_scale=1.2)\n",
        "ax = sns.heatmap(all_importance_scores.reshape(len(models), -1), annot=False, cmap=\"YlOrRd\", fmt=\".1f\", cbar=False, xticklabels=False, yticklabels=[model[0] for model in models])\n",
        "ax.set_title(\"Pooled feature importance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-8NBhu0in6A"
      },
      "source": [
        "|**Heatmap for feature importance pooled best 50 features**|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHJqH6wxin6A"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={\"figure.figsize\":(15,10)})\n",
        "sns.set(font_scale=1.2)\n",
        "\n",
        "for model_name, model in models:\n",
        "    print(model_name)\n",
        "    if model_name == \"DeepSurv\":\n",
        "        # Calculate feature importances\n",
        "        result = permutation_importance(model_instance, X_train, y_train_transformed, n_repeats=10, random_state=0)\n",
        "\n",
        "        # Get feature importance scores and their indices\n",
        "        importance_scores = result.importances_mean\n",
        "        importance_indices = np.argsort(importance_scores)[::-1][:50] # Slice the first 50 indices\n",
        "\n",
        "        # Calculate the baseline C-index\n",
        "        baseline = concordance_index_censored(y_test['vital_status'].astype(bool), y_pred_df['survival_prob'], y_test['overall_survival'])[0]\n",
        "\n",
        "\n",
        "        # Calculate the percentage reduction in C-index for each feature\n",
        "        importance_scores_pct = 100 * (baseline - result.importances_mean) / baseline\n",
        "\n",
        "        # Create a heatmap of feature importances\n",
        "        ax = sns.heatmap(importance_scores_pct[importance_indices].reshape(1, -1), annot=True, cmap=\"Blues\", fmt=\".1f\", cbar=False, xticklabels=X_train.columns[importance_indices], yticklabels=False)\n",
        "        ax.set_title(f\"Feature importance for {model_name}\")\n",
        "        plt.show()\n",
        "        \n",
        "    else:\n",
        "        # Calculate feature importances\n",
        "        result = permutation_importance(model, X_train, y_train_struct, n_repeats=10, random_state=0)\n",
        "\n",
        "        # Get feature importance scores and their indices\n",
        "        importance_scores = result.importances_mean\n",
        "        importance_indices = np.argsort(importance_scores)[::-1][:50] # Slice the first 50 indices\n",
        "\n",
        "        # Calculate the baseline C-index\n",
        "        baseline = concordance_index_censored(y_test['vital_status'].astype(bool), y_pred_df['survival_prob'], y_test['overall_survival'])[0]\n",
        "\n",
        "        # Calculate the percentage reduction in C-index for each feature\n",
        "        importance_scores_pct = 100 * (baseline - result.importances_mean) / baseline\n",
        "\n",
        "        # Create a heatmap of feature importances\n",
        "        ax = sns.heatmap(importance_scores_pct[importance_indices].reshape(1, -1), annot=False, cmap=\"Blues\", fmt=\".1f\", cbar=False, xticklabels=X_train.columns[importance_indices], yticklabels=False)\n",
        "        ax.set_title(f\"Feature importance for {model_name}\")\n",
        "        plt.xticks(rotation=90)\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQro1N8yin6A"
      },
      "source": [
        "|**plot feature importance**|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Nj5D2b8in6A"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature importance scores and their indices\n",
        "importance_scores = result.importances_mean\n",
        "indices = np.argsort(importance_scores)[::-1]\n",
        "\n",
        "# Plot feature importances\n",
        "plt.bar(range(X_train.shape[1]), importance_scores[indices])\n",
        "plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
        "plt.xlim([-1, X_train.shape[1]])\n",
        "plt.ylabel('Feature Importance')\n",
        "plt.title('Feature Importances')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvVqagPGin6A"
      },
      "source": [
        "|**plot feature importance 50 best**|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQyUgAGxin6A"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Sort feature importances\n",
        "indices = np.argsort(importance_scores)[::-1]\n",
        "\n",
        "# Set number of features to display\n",
        "n_features = 50\n",
        "\n",
        "# Plot top N features\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.barh(range(n_features), importance_scores[indices[:n_features]])\n",
        "plt.yticks(range(n_features), X.columns[indices[:n_features]])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Top {} Feature Importances'.format(n_features))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IlnL4mEin6A"
      },
      "source": [
        "|**create new X DataFrame with top ten features to use for new improved prediction modelling**|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U5fN898in6A"
      },
      "outputs": [],
      "source": [
        "# Get feature importance scores and their indices\n",
        "importance_scores = result.importances_mean\n",
        "importance_indices = np.argsort(importance_scores)[::-1][:10] # Get the top 10 important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWkaaSPHin6A"
      },
      "outputs": [],
      "source": [
        "X_important = pd.DataFrame()\n",
        "# Add the important features to the dataframe\n",
        "for i in importance_indices:\n",
        "    feature_name = X_train.columns[i]\n",
        "    X_important[feature_name] = X_train[feature_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J61BMsFin6A"
      },
      "outputs": [],
      "source": [
        "X_important"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}